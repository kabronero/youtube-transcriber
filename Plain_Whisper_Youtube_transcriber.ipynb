{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "Whisper Youtube plain transcriber\n",
        "--- \n",
        "Useful to transcript lectures, discourses, etc, where it's just one person talking and you want the full plain transcribe with no timestamps or extra stuff.\n",
        "\n",
        "Using: \n",
        "- https://github.com/openai/whisper.git \n",
        "- https://pytube.io/en/latest/ \n",
        "\n",
        "Make sure your Runtype Type is on GPU Hardware acceleration and just run each block of code in order. \n",
        "\n",
        "By **kabronero** // [Twitter](https://twitter.com/kabronero) // [Github](https://github.com/kabronero/)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3Lmjc4SkZBhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrL3hKJ9O2tS"
      },
      "outputs": [],
      "source": [
        "# First, install libraries\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pytube\n",
        "print(\"Good, now execute the next block\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, import the libraries\n",
        "\n",
        "import pytube as pt\n",
        "import whisper\n",
        "print(\"Good, now execute the next block\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zYywXBxO5sc",
        "outputId": "6c8fafba-a647-4e44-e1b0-b749484c8142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good, now execute the next block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and load the model you want to use (tiny, base, small, medium, large). \n",
        "# The smallest the model, the faster the transcript but with a larger margin of error. \n",
        "# \"medium\" is a good sweetspot.\n",
        "\n",
        "model = whisper.load_model(\"medium\")\n",
        "print(\"Good, now execute the next block\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7g_BpLuO9mA",
        "outputId": "3ee4a2ca-b6b0-4e6c-8f50-88f1b4a1137f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:19<00:00, 76.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good, now execute the next block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute to enter the Youtube URL of the video to transcribe\n",
        "\n",
        "video = input('Paste URL and press Enter: ')\n",
        "print(\"Good, now execute the next block\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1de60e-2c57-470c-8fc1-8a5505d11d9c",
        "id": "yCXsZPUcdias"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste URL and press Enter: \n",
            "Good, now execute the next block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloads the audio of the video\n",
        "\n",
        "yt = pt.YouTube(video)\n",
        "stream = yt.streams.filter(only_audio=True)[0]\n",
        "stream.download(filename=\"audio.mp3\")\n",
        "print(\"Good, now execute the next block\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWVF_XF-O7qc",
        "outputId": "e2dafa3d-8a8b-480f-abec-22b49905169c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good, now execute the next block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model and wait for the transcription! \n",
        "result = model.transcribe(\"audio.mp3\")\n",
        "f = open('result.txt', \"w\")\n",
        "f.write(result[\"text\"])\n",
        "f.close()\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "6X7oLikGPBPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7b8c36-c3c6-4585-e86d-a621b075b263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Jamf's proprietary technology has been able to deliver great results for its customers. Their platform has achieved 10 times more scan-generated installs since June 2020. Along with the technical improvements to their platform, Jamf also created an iOS 14 resources hub to share their experience and learnings with advertisers. Fetch Rewards, one of Jamf's customers, highlighted that, when you're part of such a competitive industry, innovation and speed are the key to growth. Apple's new privacy initiative had a huge impact on our industry, and having proactive partners like Jamf allowed us to be at the forefront of the change brought on by iOS 14.5, testing, learning, and evolving to continue engaging our users with relevant ads. The Jamf team is confident that these changes present a unique opportunity, and they are excited to continue working along with leading mobile marketers to deliver great results.\n"
          ]
        }
      ]
    }
  ]
}